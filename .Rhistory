for (step in 1:1) {
sess$run(train)
cat(sess$run(y_data), sess$run(W), sess$run(b), sess$run(y))
if (step %% 20 == 0)
cat(step, "-", sess$run(W), sess$run(b), "\n")
}
y_data(shape(1L))
library(tensorflow)
# Create 100 phony x, y data points, y = x * 0.1 + 0.3
x_data <- runif(5, min=0, max=1)
y_data <- x_data * 0.2 + 0.5
# Try to find values for W and b that compute y_data = W * x_data + b
# (We know that W should be 0.1 and b 0.3, but TensorFlow will
# figure that out for us.)
W <- tf$Variable(tf$random_uniform(shape(1L), -1.0, 1.0))
b <- tf$Variable(tf$zeros(shape(1L)))
y <- W * x_data + b
# Minimize the mean squared errors.
loss <- tf$reduce_mean((y - y_data) ^ 2)
optimizer <- tf$train$GradientDescentOptimizer(0.5)
train <- optimizer$minimize(loss)
# Launch the graph and initialize the variables.
sess = tf$Session()
sess$run(tf$global_variables_initializer())
# Fit the line (Learns best fit is W: 0.1, b: 0.3)
for (step in 1:1) {
sess$run(train)
cat(sess$run(W), sess$run(b), sess$run(y))
if (step %% 20 == 0)
cat(step, "-", sess$run(W), sess$run(b), "\n")
}
datafile <- read.csv("C:\Users\PraveenKumar\Downloads\Tennis2018\Tennis2018.csv", header = T, na.strings = c(""), stringsAsFactors = T))
datafile <- read.csv("C:/Users/PraveenKumar/Downloads/Tennis2018/Tennis2018.csv", header = T, na.strings = c(""), stringsAsFactors = T))
datafile <- read.csv("C:/Users/PraveenKumar/Downloads/Tennis2018/Tennis2018.csv", header = T, na.strings = c(""), stringsAsFactors = T)
rep1 <- datafile[datafile$Series == "Grand Slam"]
rep1 <- datafile[[datafile$Series == "Grand Slam"],]
rep1 <- datafile[[datafile$Series == 'Grand Slam'],]
rep1 <- datafile[datafile[[Series]] == 'Grand Slam',]
rep1 <- datafile[datafile[[Series]] == "Grand Slam",]
rep1 <- datafile[datafile[,Series] == "Grand Slam",]
rep1 <- filter(datafile, Series=="Grand Slam")
View(datafile)
rep1 <- filter(datafile, datafile$Series=="Grand Slam")
rep1
table(rep1)
finals <- filter(grandSlam, grandSlam$Round == "The Finals")
grandSlam <- as.data.frame(grandSlam)
str(grandSlam)
grandSlam <- filter(datafile, datafile$Series=="Grand Slam")
grandSlam <- as.data.frame(grandSlam)
finals <- filter(grandSlam, grandSlam$Round == "The Finals")
finals <- finals[,c(10,12,29,31,33,35,37,39)]
View(finals)
View(finals)
rep1 <- finals[,c(10,12,29,31,33,35,37,39)]
rep1 <- finals[,c(10,12,29,31,33,35,37,39)]
finals <- filter(grandSlam, grandSlam$Round == "The Finals")
rep1 <- finals[,c(10,12,29,31,33,35,37,39)]
finals
grandSlam <- filter(datafile, (datafile$Series=="Grand Slam" & datafile$Round == "The Finals")
grandSlam <- filter(datafile, (datafile$Series=="Grand Slam" & datafile$Round == "The Finals"))
grandSlam <- filter(datafile, (datafile$Series=="Grand Slam" && datafile$Round == "The Finals"))
grandSlam
datafile <- read.csv("C:/Users/PraveenKumar/Downloads/Tennis2018/Tennis2018.csv", header = T, na.strings = c(""), stringsAsFactors = T)
datafile <- read.csv("C:/Users/PraveenKumar/Downloads/Tennis2018/Tennis2018.csv", header = T, na.strings = c(""))
library(dplyr)
grandSlam <- filter(datafile, (datafile$Series=="Grand Slam" && datafile$Round == "The Finals"))
grandSlam
grandSlam <- filter(datafile, datafile$Series=="Grand Slam")
grandSlam
finals <- filter(grandSlam, grandSlam$Round == "The Finals")
finals
grandSlam <- filter(datafile, (datafile$Series=="Grand Slam" && datafile$Round == "The Final"))
grandSlam
datafile <- read.csv("C:/Users/PraveenKumar/Downloads/Tennis2018/Tennis2018.csv", header = T, na.strings = c(""))
library(dplyr)
grandSlam <- filter(datafile, (datafile$Series=="Grand Slam")
datafile <- read.csv("C:/Users/PraveenKumar/Downloads/Tennis2018/Tennis2018.csv", header = T, na.strings = c(""))
library(dplyr)
grandSlam <- filter(datafile, datafile$Series=="Grand Slam")
grandSlam
finals <- filter(grandSlam, grandSlam$Round == "The Final")
finals
rep1 <- finals[,c(10,12,29,31,33,35,37,39)]
rep1
table(rep1)
rep1 <- finals[,c(10,12,29,31,33,35,37,39)]
prop.table(table(rep1))
rep1
library(ggplot2)
qplot(Winner, data=rep1, geom="EXW", fill=gear, alpha=I(.5),
main="Distribution of Gas Milage", xlab="Winner",
ylab="Winner Bet Odds")
library(ggplot2)
qplot(Winner, data=rep1, geom="density", fill=gear, alpha=I(.5),
main="Distribution of Gas Milage", xlab="Winner",
ylab="Winner Bet Odds")
library(ggplot2)
qplot(Winner, data=rep1, geom="density", fill=EXW, alpha=I(.5),
main="Distribution of Gas Milage", xlab="Winner",
ylab="Winner Bet Odds")
library(ggplot2)
qplot(Winner, data=rep1, geom="density", fill=EXW, alpha=I(.75),
main="Distribution of Gas Milage", xlab="Winner",
ylab="Winner Bet Odds")
hist(rep1$EXW)
data <- read.csv(file.choose(), header = T)
install.packages("naivebayes")
library(naivebayes)
library(dplyr)
library(ggplot2)
library(psych)
str(data)
View(data)
library(naivebayes)
library(dplyr)
library(ggplot2)
library(psych)
data <- read.csv(file.choose(), header = T, stringsAsFactors = T)
str(data)
data$Survived <- as.factor(data$Survived)
data$Pclass <- as.factor(data$Pclass)
data$SibSp <- as.factor(data$SibSp)
data$Parch <- as.factor(data$Parch)
str(data)
#Visualization
pairs.panels(data[-2])
ggplot(aes(x=Survived, y=Pclass, fill=Survived)) +
geom_boxplot() +
ggtitle("Box Plot")
data %>%
ggplot(aes(x=Survived, y=Pclass, fill=Survived)) +
geom_boxplot() +
ggtitle("Box Plot")
data %>%
ggplot(aes(x=Survived, y=Age, fill=Survived)) +
geom_boxplot() +
ggtitle("Box Plot")
data %>%
ggplot(aes(x=Survived, y=Fare, fill=Survived)) +
geom_boxplot() +
ggtitle("Box Plot")
data %>% ggplot(aes(x=Age, fill=Survived)) +
geom_density() +
ggtitle("Density Plot")
data %>% ggplot(aes(x=Age, fill=Survived)) +
geom_density(alpha=0.8) +
ggtitle("Density Plot")
flag <- sample(2, nrow(data), replace = T, prob = c(0.8,0.2))
set.seed(1234)
flag <- sample(2, nrow(data), replace = T, prob = c(0.8,0.2))
train <- data[flag==1,]
test <- data[flag==2,]
#Naive Model
model <- naive_bayes(Survived ~ ., data = train)
model
plot(model)
#Prediction
p <- predict(model, train, type = 'prob')
head(cbind(p,train))
p1 <- predict(model, train)
tab1 <- table(p1, train$Survived)
p1 <- predict(model, train)
(tab1 <- table(p1, train$Survived))
p2 <- predict(model, test)
(tab1 <- table(p2, test$Survived))
p2 <- predict(model, test)
(tab2 <- table(p2, test$Survived))
1 - sum(diag(tab2)) / sum(tab2)
p1 <- predict(model, train)
(tab1 <- table(p1, train$Survived))
1 - sum(diag(tab1)) / sum(tab1)
#Naive Model
model <- naive_bayes(Survived ~ ., data = train, usekernel = T)
p1 <- predict(model, train)
(tab1 <- table(p1, train$Survived))
1 - sum(diag(tab1)) / sum(tab1)
p2 <- predict(model, test)
(tab2 <- table(p2, test$Survived))
1 - sum(diag(tab2)) / sum(tab2)
mnist <- read.csv(file.choose(), header=T)
str(mnist)
mnist$label <- as.factor(mnist$label)
table(mnist$label)
View(mnist)
columsKeep <- names(which(colSums(mnist[, -1]) > 0))
which(colSums(mnist[, -1]) > 0)
columsKeep
mnist <- mnist[c("label", columsKeep)]
mnist
head(mnist)
library(caret)
library(e1071)
library(ggfortify)
install.packages("ggfortify")
library(ggfortify)
set.seed(1337)
index <- createDataPartition(mnist$label, p = .25, list = FALSE)
index
pca <- prcomp(mnist[index, -1], scale. = F, center = F)
autoplot(pca, data=mnist[index,], colour='label')
screeplot(pca, type="lines", npcs = 50)
?createDataPartition
?prcomp
var.pca <- pca$sdev ^ 2
x.var.pca <- var.pca / sum(var.pca)
x.var.pca
cum.var.pca <- cumsum(x.var.pca)
cum.var.pca
plot(cum.var.pca[1:200],xlab="No. of principal components",
ylab="Cumulative Proportion of variance explained", ylim=c(0,1), type='b')
performanceSVM <- function(model, test, isTest) {
results <- predict(model, test)
if (isTest) {
cm <- confusionMatrix(results, mnist[-index, 1])
} else {
cm <- confusionMatrix(results, mnist[index, 1])
}
return(cm$overall[c(1:2)])
}
pcs <- 10
train <- as.matrix(mnist[index,-1]) %*% pca$rotation[,1:pcs]
test <- as.matrix(mnist[-index,-1]) %*% pca$rotation[,1:pcs]
model <- svm(train, mnist[index, 1], kernel="linear")
(performanceSVM(model, test, TRUE))
minPCs <- 2
maxPCs <- 100
results <- data.frame()
kernels <- c("linear", "polynomial", "radial", "sigmoid")
for (pcs in minPCs:maxPCs) {
print(paste("Building models with", pcs, "PCs"))
train <- as.matrix(mnist[index,-1]) %*% pca$rotation[,1:pcs]
test <- as.matrix(mnist[-index,-1]) %*% pca$rotation[,1:pcs]
for (k in 1:length(kernels)) {
start_time <- Sys.time()
model <- svm(train, mnist[index, 1], kernel=kernels[k])
end_time <- Sys.time()
start_time_E <- Sys.time()
p <- performanceSVM(model, test, TRUE)
end_time_E <- Sys.time()
df <- data.frame(t(p))
p <- performanceSVM(model, train, FALSE)
d <- data.frame(t(p))
names(d) <- c("AccuracyTrain", "KappaTrain")
df <- cbind(d, df)
df$Kernel <- kernels[k]
df$PCs <- pcs
df$RuntimeM <- end_time - start_time
df$RuntimeE <- end_time_E - start_time_E
results <- rbind(results, df)
}
}
age <- c(10,37,NA)
age <- age[na.rm = TRUE]
age
age <- na.omit(age)
age
(age <- na.omit(age))
install.packages("data.table")
library(data.table)
outlierReplace(age, which(age == NA),max(age))
newdata <- na.omit(age)
newdata
newdata[1]
newdata[3]
setwd("C:/Study Materials/Kaggle/CA1 ADM")
setwd("C:/Study Materials/Kaggle/CA1 ADM")
hrdata <- read.csv("ADMCA1Data.csv", header = T, stringsAsFactors = T)
set.seed(17125201)
my_dataset <- hrdata[order(runif(600)), ]
#let's remove ID, we probably don't want that:
my_dataset <- my_dataset[-10]
#Unfortunately, due to a technical error, 3 columns of the data were lost :(
#HR blamed IT, IT blamed HR, your manager will blame you, so let's just hope those columns weren't important!
col1 <- round(runif(1)*32)+2 #the +2 protects the Age and Attrition Variables
col2 <- round(runif(1)*31)+2
col3 <- round(runif(1)*30)+2
cols <- names(my_dataset)
print(paste("I lost: ", cols[col1], ",", cols[col2], ",", cols[col3]))
#"I lost:  StandardHours , OverTime , Gender"
my_dataset <- my_dataset[-col1]
my_dataset <- my_dataset[-col2]
my_dataset <- my_dataset[-col3]
#if you want to use something other than R save your dataset:
write.csv(file="mydata.csv", my_dataset, row.names = F)
#Now please begin, and good luck!
#Because you lost 3 columns, some models may/may not work as well,
#don't worry about this, I will control for it in the grading!
mydata <- read.csv("mydata.csv", header = T, stringsAsFactors = T)
str(mydata)
mydata$Education <- factor(mydata$Education, levels = c(1,2,3,4,5), labels = c("BC", "C", "B", "M", "D"))
mydata$EnvironmentSatisfaction <- factor(mydata$EnvironmentSatisfaction, levels = c(1,2,3,4), labels = c("Low", "Medium", "High", "Very High"))
mydata$JobInvolvement <- factor(mydata$JobInvolvement, levels = c(1,2,3,4), labels = c("Low", "Medium", "High", "Very High"))
mydata$JobSatisfaction <- factor(mydata$JobSatisfaction, levels = c(1,2,3,4), labels = c("Low", "Medium", "High", "Very High"))
mydata$PerformanceRating <- factor(mydata$PerformanceRating, levels = c(1,2,3,4), labels = c("Low", "Good", "Excellent", "Outstanding"))
mydata$RelationshipSatisfaction <- factor(mydata$RelationshipSatisfaction, levels = c(1,2,3,4), labels = c("Low", "Medium", "High", "Very High"))
mydata$WorkLifeBalance <- factor(mydata$WorkLifeBalance, levels = c(1,2,3,4), labels = c("Bad", "Good", "Better", "Best"))
summary(mydata)
#To remove the missing factors
mydata$PerformanceRating <- factor(mydata$PerformanceRating)
table(mydata$JobRole)
#To remove unwanted columns
mydata <- mydata[,-8]
mydata <- mydata[,-18]
#Missing Values
sapply(mydata,function(x) sum(is.na(x)))
#outliers
boxplot(subset(mydata, select=c(1,6,18,20,21:24))) #nothing too obvious here
boxplot(subset(mydata, select=c(26:29))) #May be something in years at company
boxplot(mydata$DailyRate)
boxplot(mydata$HourlyRate)
boxplot(mydata$MonthlyIncome) #possibly some here too
boxplot(mydata$MonthlyRate)
#i prefer the attrition factor as 0/1 for convenience
mydata$Attrition <- factor(mydata$Attrition, labels=c(0,1), levels=c("No", "Yes"))
#F2: class balance:
table(mydata$Attrition) #fairly imbalanced
#F3: train and test
#2 options here stratified, or under/over sampling
library(caret)
sample <- createDataPartition(mydata$Attrition, p = .75, list = FALSE)
train <- mydata[sample, ]
test <- mydata[-sample, ]
#F4: benchmarks
#1 everyone stays:
str(test$Attrition) # remain (Y) is 0
b1 <- rep(0, dim(test)[1])
(accuracyB1 <- 1 - mean(b1 != test$Attrition))
#here we really see the effect of the class imbalance!
#2 those who have higher satisfaction are less likely to leave
str(test$JobSatisfaction)
b2 <- rep(0, dim(test)[1])
b2[test$JobSatisfaction == 'Low'] <- 1
b2[test$JobSatisfaction == 'Medium'] <- 1
(accuracyB2 <- 1 - mean(b2 != test$Attrition))
#End Foundations
##########################################
#Start of Question: B1
#work out what columns we have:
str(subset(mydata, select=c(1,4,5,10,12,15:17,19,22:24,26:29)))
#now correlation test
cor(subset(mydata, select=c(1,4,5,10,12,15:17,19,22:24,26:29)))
#End of Question
##########################################
#Start of Question: B2
boxplot(mydata$ï..Age ~ mydata$Attrition)
spineplot(mydata$Education, mydata$Attrition)
#End of Question
##########################################
#Start of Question: B3
#I don't have gender, or overtime, so selecting marital status
spineplot(mydata$Gender, mydata$Attrition) #single people seem to leave more often
boxplot(mydata$DistanceFromHome ~ mydata$Gender) #surprisingly there doesn't appear to be much difference in commute distance by martial status
#End of Question
##########################################
#Start of Question: B4
plot(mydata$ï..Age, mydata$MonthlyRate, xlab = 'Age', ylab = 'Monthly Rate', main = "Scatter plot of Age vs. Monthly Rate")
plot(mydata$ï..Age, mydata$DailyRate, main = "Scatter plot of Age vs. Daily Rate")
plot(mydata$ï..Age, mydata$HourlyRate, main = "Scatter plot of Age vs. Hourly Rate")
#doesn't look like it
#End of Question
##########################################
#Start of Question: B5
(counts <- table(mydata$Attrition, mydata$JobSatisfaction))
row.names(counts) <- c("Remained", "Left")
barplot(counts, main="Attrition Distribution by Job Satisfaction", legend = row.names(counts))
#or if you want something a little easier to read
barplot(counts, main="Attrition Distribution by Job Satisfaction", xlab="Job Satistfaction", col=c("darkblue","red"), legend = rownames(counts), beside=T)
#End of Question
##########################################
#Start of Question: I1
library(C50)
cFifty <- C5.0(Attrition ~., data=train, trials=100)
#if you saw: c50 code called exit with value 1 you had unused levels still in your factors or some single level factors (e.g. over 18)
cFiftyPrediction <- predict(cFifty, newdata = test[, -2])
(cFiftyAccuracy <- 1- mean(cFiftyPrediction != test$Attrition))
#0.8724832
cFiftyAccuracy - accuracyB1
#0.02684564 -- marginally better than always predicting remain
cFiftyAccuracy - accuracyB2
#0.2751678 -- a lot beter than just assuming that higher statisfaction are less likely to leave
#End of Question
##########################################
#Start of Question: I2
#kNN requires normalised data
library(class)
normalize <- function(x) { return ((x - min(x)) / (max(x) - min(x))) }
#now let's normalise our dataset so that calculating the distances in the feature space makes sense
my_dataset_n <- subset(mydata, select=c(1,4,5,10,12,15:17,19,22:24,26:29)) #get the numerical for normalisation -- kNN also doesn't support levelled factors either
str(my_dataset_n)
my_dataset_n <- as.data.frame(lapply(my_dataset_n, normalize)) #normalise
summary(my_dataset_n) #all our numericals are normalised, our categoricals are untouched
my_dataset_n <- cbind(my_dataset[,1],mydata[,2],my_dataset_n[,2:16])
colnames(my_dataset_n)[1:2] <- c("Age", "Attrition")
my_dataset_n <- my_dataset_n[,-11]
#re make train and test note we can retain the original distribution if we choose to
train_n <- my_dataset_n[sample, ]
test_n <- my_dataset_n[-sample, ]
#Missing Values
sapply(test_n,function(x) sum(is.na(x)))
#different ways to determine k
k1 <- round(sqrt(dim(train_n)[1])) #sqrt of number of instances
k2 <- round(sqrt(dim(train_n)[2])) #sqrt of number of attributes
k3 <- 7 #a number between 3 and 10
knn1 <- knn(train = train_n, test = test_n, cl = train$Attrition, k=k1)
knn2 <- knn(train = train_n, test = test_n, cl = train$Attrition, k=k2)
knn3 <- knn(train = train_n, test = test_n, cl = train$Attrition, k=k3)
(knn1Acc <- 1- mean(knn1 != test$Attrition))
(knn2Acc <- 1- mean(knn2 != test$Attrition))
(knn3Acc <- 1- mean(knn3 != test$Attrition))
knn1Acc - accuracyB1 #same as benchmark
knn2Acc - accuracyB1 #worse then benchmark
knn3Acc - accuracyB1 #same as benchmark
dim(my_dataset_n)
my_dataset_c <- subset(mydata, select=c(2,3,5,7:9,11:15,20:22,25)) #isolate the categoricals
my_dataset_n <- cbind(my_dataset_n, my_dataset_c) #put them back together if you want to include the categoricals in a later question
#don't run this line if you are going to do A1! PCA will break.
logit <- glm(train$Attrition ~.,family=binomial(link='logit'),data=train)
summary(logit)
anova(logit, test="Chisq")
pR2(logit)
results.1.logit <- predict(logit,newdata=test[,-2],type='response')
results.1.logit <- ifelse(results.1.logit > 0.5,1,0)
(logitAcc1 <- 1- mean(results.1.logit != test$Attrition))
results.2.logit <- predict(logit,newdata=test[,-2],type='response')
results.2.logit <- ifelse(results.2.logit > 0.6,1,0)
(logitAcc2 <- 1- mean(results.2.logit != test$Attrition))
results.3.logit <- predict(logit,newdata=test[,-2],type='response')
results.3.logit <- ifelse(results.3.logit > 0.75,1,0)
(logitAcc3 <- 1- mean(results.3.logit != test$Attrition))
logitAcc1 - accuracyB1 #better than benchmark
logitAcc2 - accuracyB1 #better then benchmark
logitAcc3 - accuracyB1 #better (but not as much as above 2) as benchmark
library(randomForest)
forest <- randomForest(Attrition ~ ., data=train, importance=TRUE, ntree=2000)
varImpPlot(forest)
#while we're at it, let's also see how well it does (for A3)
forestPrediction <- predict(forest, test[,-2], type = "class")
(forestAcc <- 1- mean(forestPrediction != test$Attrition))
setwd("C:/Study Materials/Kaggle/heart-disease-uci")
heartdata <- read.csv("heart.csv", header = T)
View(heartdata)
str(heartdata)
heartdata$sex <- factor(heartdata$sex, levels = c(0,1), labels = c("Female", "Male"))
heartdata$cp < as.factor(heartdata$cp)
heartdata$cp <- as.factor(heartdata$cp)
heartdata$sex <- factor(heartdata$sex, levels = c(0,1), labels = c("Female", "Male"))
heartdata$cp <- as.factor(heartdata$cp)
heartdata$fbs <- factor(heartdata$fbs, levels = c(0,1), labels = c("False", "True"))
heartdata$exang <- factor(heartdata$exang, levels = c(0,1), labels = c("No", "Yes"))
heartdata$ca <- as.factor(heartdata$ca)
heartdata$thal <- factor(heartdata$fbs, levels = c(1,2,3), labels = c("Normal", "Fixed Defect", "Reversable Defect"))
str(heartdata)
heartdata$target <- as.factor(heartdata$target)
summary(heartdata)
setwd("C:/Study Materials/Kaggle/heart-disease-uci")
heartdata <- read.csv("heart.csv", header = T)
#Setting random seed
set.seed(1712)
str(heartdata)
#Cleaning the data
#Identifying the correct data types
heartdata$sex <- factor(heartdata$sex, levels = c(0,1), labels = c("Female", "Male"))
heartdata$cp <- as.factor(heartdata$cp)
heartdata$fbs <- factor(heartdata$fbs, levels = c(0,1), labels = c("False", "True"))
heartdata$exang <- factor(heartdata$exang, levels = c(0,1), labels = c("No", "Yes"))
heartdata$ca <- as.factor(heartdata$ca)
heartdata$thal <- factor(heartdata$fbs, levels = c(1,2,3), labels = c("Normal", "Fixed Defect", "Reversable Defect"))
heartdata$target <- as.factor(heartdata$target)
summary(heartdata)
setwd("C:/Study Materials/Kaggle/heart-disease-uci")
heartdata <- read.csv("heart.csv", header = T)
#Setting random seed
set.seed(1712)
str(heartdata)
#Cleaning the data
#Identifying the correct data types
heartdata$sex <- factor(heartdata$sex, levels = c(0,1), labels = c("Female", "Male"))
heartdata$cp <- as.factor(heartdata$cp)
heartdata$fbs <- factor(heartdata$fbs, levels = c(0,1), labels = c("False", "True"))
heartdata$exang <- factor(heartdata$exang, levels = c(0,1), labels = c("No", "Yes"))
heartdata$ca <- as.factor(heartdata$ca)
heartdata$thal <- factor(heartdata$thal, levels = c(1,2,3), labels = c("Normal", "Fixed Defect", "Reversable Defect"))
heartdata$target <- as.factor(heartdata$target)
summary(heartdata)
#To find the missing data
sapply(heartdata, function(x) sum(is.na(x)))
heartdata_n <- na.omit(heartdata)
sapply(heartdata_n, function(x) sum(is.na(x)))
#To rename the column name
names(heartdata_n[1]) <- 'Age'
View(heartdata_n)
#To rename the column name
colnames(heartdata_n[1]) <- c("Age")
summary(heartdata_n)
#To rename the column name
colnames(heartdata_n)[1] <- c("Age")
View(heartdata_n)
table(heartdata_n$target)
idx <- createDataPartition(heartdata_n, p=0.75, list = FALSE)
library(caret)
idx <- createDataPartition(heartdata_n, p=0.75, list = FALSE)
train <- heartdata_n[idx,]
test <- heartdata_n[-idx,]
idx <- createDataPartition(heartdata_n$target, p=0.75, list = FALSE)
train <- heartdata_n[idx,]
test <- heartdata_n[-idx,]
#Let's test the accuracy with everyone has the heart disease
a1 <- rep(1, dim(heartdata_n)[1])
#Let's test the accuracy with everyone has the heart disease
a1 <- rep(1, dim(test)[1])
(accu1 <- 1-mean(a1 == test$target))
