hypmodel <- h2o.randomForest(x = names(h2otrain[-476]), y = 476, training_frame = h2otrain)
pred <- h2o.predict(hypmodel, h2otest)
pred <- as.data.frame(pred)
View(pred)
#  results.DL <- ifelse(pred$predict > 0.5,1,0)
dlPerformance <- 1 - mean(pred$predict != tes1$Score)
dlPerformance
hypmodel <- h2o.randomForest(x = names(h2otrain[-476]), y = 476, training_frame = h2otrain,
nfolds = 3, ignore_const_cols = TRUE, ntrees = 50, max_depth = 7)
pred <- h2o.predict(hypmodel, h2otest)
pred <- as.data.frame(pred)
#  results.DL <- ifelse(pred$predict > 0.5,1,0)
(1 - mean(pred$predict != tes1$Score))
hypmodel <- h2o.randomForest(x = names(h2otrain[-476]), y = 476, training_frame = h2otrain,
nfolds = 2, ignore_const_cols = TRUE, ntrees = 30, max_depth = 3)
pred <- h2o.predict(hypmodel, h2otest)
pred <- as.data.frame(pred)
#  results.DL <- ifelse(pred$predict > 0.5,1,0)
(1 - mean(pred$predict != tes1$Score))
hypmodel <- h2o.randomForest(x = names(h2otrain[-476]), y = 476, training_frame = h2otrain,
nfolds = 4, ignore_const_cols = TRUE, ntrees = 50, max_depth = 5,
mtries = -1, sample_rate = 0.6320000291)
pred <- h2o.predict(hypmodel, h2otest)
pred <- as.data.frame(pred)
(1 - mean(pred$predict != tes1$Score))
hypmodel <- h2o.randomForest(x = names(h2otrain[-476]), y = 476, training_frame = h2otrain,
nfolds = 4, ignore_const_cols = TRUE, ntrees = 70, max_depth = 20,
mtries = -1, sample_rate = 0.6320000291)
pred <- h2o.predict(hypmodel, h2otest)
pred <- as.data.frame(pred)
(1 - mean(pred$predict != tes1$Score))
hypmodel <- h2o.randomForest(x = names(h2otrain[-476]), y = 476, training_frame = h2otrain,
nfolds = 5, ignore_const_cols = TRUE, ntrees = 100, max_depth = 20,
mtries = -1, sample_rate = 0.6320000291)
pred <- h2o.predict(hypmodel, h2otest)
pred <- as.data.frame(pred)
(1 - mean(pred$predict != tes1$Score))
hypmodel <- h2o.randomForest(x = names(h2otrain[-476]), y = 476, training_frame = h2otrain,
nfolds = 5, ignore_const_cols = TRUE, ntrees = 100, max_depth = 20,
mtries = -1, sample_rate = 0.6320000291, nbins = 20)
pred <- h2o.predict(hypmodel, h2otest)
pred <- as.data.frame(pred)
(1 - mean(pred$predict != tes1$Score))
hyper_params <- list( mtries = c(10, 40, 50) )
grid <- h2o.grid(x = names(h2otrain[-476]), y = 476, training_frame = h2otrain,
algorithm = "drf", grid_id = "covtype_grid", hyper_params = hyper_params,
search_criteria = list(strategy = "Cartesian"), seed = 1234)
for (model_id in grid@model_ids){
model <- h2o.getModel(model_id)
pred <- h2o.predict(hypmodel, h2otest)
pred <- as.data.frame(pred)
#  results.DL <- ifelse(pred$predict > 0.5,1,0)
dlPerformance <- 1 - mean(pred$predict != tes1$Score)
dlPerf <- rbind(dlPerf, dlPerformance)
}
dlPerf <- c()
for (model_id in grid@model_ids){
model <- h2o.getModel(model_id)
pred <- h2o.predict(hypmodel, h2otest)
pred <- as.data.frame(pred)
#  results.DL <- ifelse(pred$predict > 0.5,1,0)
dlPerformance <- 1 - mean(pred$predict != tes1$Score)
dlPerf <- rbind(dlPerf, dlPerformance)
}
#The best accuracy
(bestDL <- max(dlPerf))
hyper_params <- list( mtries = c(10, 60, 80) )
grid <- h2o.grid(x = names(h2otrain[-476]), y = 476, training_frame = h2otrain,
algorithm = "drf", grid_id = "covtype_grid", hyper_params = hyper_params,
search_criteria = list(strategy = "Cartesian"), seed = 1234)
dlPerf <- c()
for (model_id in grid@model_ids){
model <- h2o.getModel(model_id)
pred <- h2o.predict(hypmodel, h2otest)
pred <- as.data.frame(pred)
#  results.DL <- ifelse(pred$predict > 0.5,1,0)
dlPerformance <- 1 - mean(pred$predict != tes1$Score)
dlPerf <- rbind(dlPerf, dlPerformance)
}
#The best accuracy
(bestDL <- max(dlPerf))
grid <- h2o.grid(x = names(h2otrain[-476]), y = 476, training_frame = h2otrain,
algorithm = "drf", grid_id = "covtype_grid", hyper_params = hyper_params,
search_criteria = list(strategy = "RandomDiscrete"), seed = 1234)
dlPerf <- c()
for (model_id in grid@model_ids){
model <- h2o.getModel(model_id)
pred <- h2o.predict(hypmodel, h2otest)
pred <- as.data.frame(pred)
#  results.DL <- ifelse(pred$predict > 0.5,1,0)
dlPerformance <- 1 - mean(pred$predict != tes1$Score)
dlPerf <- rbind(dlPerf, dlPerformance)
}
#The best accuracy
(bestDL <- max(dlPerf))
hyper_params <- list( mtries = c(10, 40, 50) )
grid <- h2o.grid(x = names(h2otrain[-476]), y = 476, training_frame = h2otrain,
algorithm = "drf", grid_id = "covtype_grid", hyper_params = hyper_params,
search_criteria = list(strategy = "RandomDiscrete"), seed = 1234)
dlPerf <- c()
for (model_id in grid@model_ids){
model <- h2o.getModel(model_id)
pred <- h2o.predict(hypmodel, h2otest)
pred <- as.data.frame(pred)
#  results.DL <- ifelse(pred$predict > 0.5,1,0)
dlPerformance <- 1 - mean(pred$predict != tes1$Score)
dlPerf <- rbind(dlPerf, dlPerformance)
}
#The best accuracy
(bestDL <- max(dlPerf))
library(jpeg)
library(EBImage)
#Path of the data from where the images should be fetched
Data.dir <- 'C:/Study Materials/Machine Learning/Project/blood-cells/dataset-master/dataset-master/JPEGTest'
#Get all the file names in a vector
pic1 <- as.vector(list.files(path = Data.dir, full.names = TRUE, include.dirs = FALSE))
mydata <- c()
#Read al the JPEG files
for (i in 1:length(pic1)) {mydata[[i]] <- readJPEG(pic1[[i]])}
#Resize the images to 28*28 and convert them to a grayscale image
for (i in 1:length(mydata)) {mydata[[i]] <- resize(Image(data = mydata[[i]], dim = dim(mydata[[i]]), colormode = "Grayscale"), 28, 28)}
#Getting the features of the images in a vector
for (i in 1:length(mydata)) {mydata[[i]] <- as.vector(mydata[[i]])}
#Converting the vector to a data frame for further processing
dfr <- as.data.frame(do.call(rbind, mydata))
#Read the labels of the images from a csv
lab <- read.csv(file = "C:/Study Materials/Machine Learning/Project/blood-cells/dataset-master/dataset-master/labelsTest.csv", header = T)
#Merge the label vector to the data frame we have created earlier
dfr <- data.frame(cbind(lab$Category, dfr))
table(dfr$lab.Category)
dfr$lab.Category <- factor(dfr$lab.Category)
plot(dfr$lab.Category, xlab = 'Class',ylab = 'Count of Images', lwd = 2, col="Navy",main = "Class Imbalance")
#To make the dataset balanced
library(UBL)
df <- SmoteClassif(lab.Category ~ ., dfr, C.perc = "balance", repl = FALSE)
#Changing the labels of the dependent variable
df$lab.Category <- as.numeric(factor(df$lab.Category, levels = c("BASOPHIL", "EOSINOPHIL", "LYMPHOCYTE", "MONOCYTE", "NEUTROPHIL"), labels = c(0,1,2,3,4)))
names(df)[1] <- c("Label")
plot(df$Label, xlab = 'Class',ylab = 'Count of Images', lwd = 2, col="Navy",main = "Class Balance")
str(df)
#Check if any feature could be removed
columnsKeep <- names(which(colSums(df[,-1]) > 0))
df <- df[c("Label", columnsKeep)]
library(caret)
library(e1071)
library(ggfortify)
set.seed(17125201)
idx <- createDataPartition(df$Label, p=0.75, list = FALSE)
#PCA on the features
pca <- prcomp(df[idx,-1], scale. = F, center = F)
autoplot(pca, data = df[idx,], colour='Label')
screeplot(pca, type = "lines", npcs = 70, main = 'Screeplot of PCA')
var.pca <- pca$sdev ^ 2
x.var.pca <- var.pca/sum(var.pca)
cum.var.pca <- cumsum(x.var.pca)
#Graph to determine the number of pcs required
plot(cum.var.pca[1:100], xlab = "No. of PCs",
ylab = "Cumulative Proportion of variance explained", ylim = c(0,1), type = 'b')
#PCA rotation on the data
pcs <- 30
indata <- as.data.frame(as.matrix(df[,-1]) %*% pca$rotation[,1:pcs])
indata <- data.frame(cbind(df[,1], indata))
names(indata)[1] <- c("Label")
hist(indata$PC6, xlab = 'PC6', main = "Normal Distribution of PC6", breaks = 25, col = rgb(0.1, 0.1, 0.9, 0.9))
qqnorm(indata$PC6)
#Splitting the data into train and test
train <- indata[idx,]
test <- indata[-idx,]
#SVM model
# test$Label <- factor(test$Label)
train$Label <- factor(train$Label)
svmModel <- svm(train[,-1], indata[idx,1], kernel = "polynomial")
results <- predict(svmModel,test[,-1])
#Confusion Matrix
a <- (confusionMatrix(results, indata[-idx,1]))
plot(a$table, xlab = 'Predicted', ylab = 'Observed', main = 'Confusion Matrix', col = 'Navy')
str(a$byClass[,c(5,6,7,11)])
library(ggplot2)
ggplot(data =  a$table, mapping = aes(x = Prediction, y = Reference))+
geom_tile(aes(fill = Freq), colour = "white") +
geom_text(aes(label = sprintf("%1.0f",Freq)), vjust = 1) +
scale_fill_gradient(low = "Sky Blue", high = "Blue",trans = "log") +
ggtitle("Confusion Matrix of SVM") +
labs(y = 'Observed Class', x='Predicted Class')
library(jpeg)
library(EBImage)
#Path of the data from where the images should be fetched
Data.dir <- 'C:/Study Materials/Machine Learning/Project/blood-cells/dataset-master/dataset-master/JPEGTest'
#Get all the file names in a vector
pic1 <- as.vector(list.files(path = Data.dir, full.names = TRUE, include.dirs = FALSE))
mydata <- c()
#Read al the JPEG files
for (i in 1:length(pic1)) {mydata[[i]] <- readJPEG(pic1[[i]])}
#Resize the images to 28*28 and convert them to a grayscale image
for (i in 1:length(mydata)) {mydata[[i]] <- resize(Image(data = mydata[[i]], dim = dim(mydata[[i]]), colormode = "Grayscale"), 28, 28)}
#Getting the features of the images in a vector
for (i in 1:length(mydata)) {mydata[[i]] <- as.vector(mydata[[i]])}
#Converting the vector to a data frame for further processing
dfr <- as.data.frame(do.call(rbind, mydata))
#Read the labels of the images from a csv
lab <- read.csv(file = "C:/Study Materials/Machine Learning/Project/blood-cells/dataset-master/dataset-master/labelsTest.csv", header = T)
#Merge the label vector to the data frame we have created earlier
dfr <- data.frame(cbind(lab$Category, dfr))
table(dfr$lab.Category)
dfr$lab.Category <- factor(dfr$lab.Category)
plot(dfr$lab.Category, xlab = 'Class',ylab = 'Count of Images', lwd = 2, col="Navy",main = "Class Imbalance")
#To make the dataset balanced
library(UBL)
df <- SmoteClassif(lab.Category ~ ., dfr, C.perc = "balance", repl = FALSE)
#Changing the labels of the dependent variable
df$lab.Category <- as.numeric(factor(df$lab.Category, levels = c("BASOPHIL", "EOSINOPHIL", "LYMPHOCYTE", "MONOCYTE", "NEUTROPHIL"), labels = c(0,1,2,3,4)))
names(df)[1] <- c("Label")
plot(df$Label, xlab = 'Class',ylab = 'Count of Images', lwd = 2, col="Navy",main = "Class Balance")
dfr <- as.data.frame(do.call(rbind, mydata))
#Read the labels of the images from a csv
lab <- read.csv(file = "C:/Study Materials/Machine Learning/Project/blood-cells/dataset-master/dataset-master/labelsTest.csv", header = T)
#Merge the label vector to the data frame we have created earlier
dfr <- data.frame(cbind(lab$Category, dfr))
table(dfr$lab.Category)
dfr$lab.Category <- factor(dfr$lab.Category)
plot(dfr$lab.Category, xlab = 'Class',ylab = 'Count of Images', lwd = 2, col="Navy",main = "Class Imbalance")
#To make the dataset balanced
library(UBL)
df <- SmoteClassif(lab.Category ~ ., dfr, C.perc = "balance", repl = FALSE)
#Changing the labels of the dependent variable
df$lab.Category <- factor(df$lab.Category, levels = c("BASOPHIL", "EOSINOPHIL", "LYMPHOCYTE", "MONOCYTE", "NEUTROPHIL"), labels = c(0,1,2,3,4))
names(df)[1] <- c("Label")
plot(df$Label, xlab = 'Class',ylab = 'Count of Images', lwd = 2, col="Navy",main = "Class Balance")
#Check if any feature could be removed
columnsKeep <- names(which(colSums(df[,-1]) > 0))
df <- df[c("Label", columnsKeep)]
library(caret)
library(e1071)
library(ggfortify)
set.seed(17125201)
idx <- createDataPartition(df$Label, p=0.75, list = FALSE)
#PCA on the features
pca <- prcomp(df[idx,-1], scale. = F, center = F)
autoplot(pca, data = df[idx,], colour='Label')
screeplot(pca, type = "lines", npcs = 70, main = 'Screeplot of PCA')
var.pca <- pca$sdev ^ 2
x.var.pca <- var.pca/sum(var.pca)
cum.var.pca <- cumsum(x.var.pca)
#Graph to determine the number of pcs required
plot(cum.var.pca[1:100], xlab = "No. of PCs",
ylab = "Cumulative Proportion of variance explained", ylim = c(0,1), type = 'b')
#PCA rotation on the data
pcs <- 30
indata <- as.data.frame(as.matrix(df[,-1]) %*% pca$rotation[,1:pcs])
indata <- data.frame(cbind(df[,1], indata))
names(indata)[1] <- c("Label")
hist(indata$PC6, xlab = 'PC6', main = "Normal Distribution of PC6", breaks = 25, col = rgb(0.1, 0.1, 0.9, 0.9))
qqnorm(indata$PC6)
#Splitting the data into train and test
train <- indata[idx,]
test <- indata[-idx,]
#SVM model
# test$Label <- factor(test$Label)
train$Label <- factor(train$Label)
svmModel <- svm(train[,-1], indata[idx,1], kernel = "polynomial")
results <- predict(svmModel,test[,-1])
#Confusion Matrix
a <- (confusionMatrix(results, indata[-idx,1]))
plot(a$table, xlab = 'Predicted', ylab = 'Observed', main = 'Confusion Matrix', col = 'Navy')
str(a$byClass[,c(5,6,7,11)])
library(ggplot2)
ggplot(data =  a$table, mapping = aes(x = Prediction, y = Reference))+
geom_tile(aes(fill = Freq), colour = "white") +
geom_text(aes(label = sprintf("%1.0f",Freq)), vjust = 1) +
scale_fill_gradient(low = "Sky Blue", high = "Blue",trans = "log") +
ggtitle("Confusion Matrix of SVM") +
labs(y = 'Observed Class', x='Predicted Class')
a
#Deep Learning using mxnet
# cran <- getOption("repos")
# cran["dmlc"] <- "https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/CRAN/"
# options(repos = cran)
# install.packages("mxnet")
library(mxnet)
library(mlbench)
indata$Label <- factor(indata$Label)
data(indata, package = "mlbench")
train.ind <- seq(1,344,4)
str(train)
train.x <- data.matrix(indata[-train.ind, -1])
train.y <- indata[-train.ind,1]
test.x <- data.matrix(indata[train.ind, -1])
test.y <- indata[train.ind,1]
data <- mx.symbol.Variable("data")
fc1 <- mx.symbol.FullyConnected(data, num_hidden = 128, name = "fc1")
act1 <- mx.symbol.Activation(fc1, name="relu1", act_type="relu")
fc2 <- mx.symbol.FullyConnected(act1, name="fc2", num_hidden=64)
act2 <- mx.symbol.Activation(fc2, name="relu2", act_type="relu")
fc3 <- mx.symbol.FullyConnected(act2, name="fc3", num_hidden=10)
softmax <- mx.symbol.SoftmaxOutput(fc3, name="sm")
#lro <- mx.symbol.LinearRegressionOutput(fc1)
mx.set.seed(0)
# mnetModel <- mx.mlp(train.x, train.y, hidden_node = 10, out_node = 2, array.batch.size=10, momentum=0.9,
#                     out_activation = "softmax", learning.rate = 0.07, num.round=30,
#                     eval.metric=mx.metric.accuracy)
mnetModel <- mx.model.FeedForward.create(softmax, X = train.x, y = train.y, array.batch.size=50,
ctx = mx.cpu(),momentum = 0.9,
learning.rate = 0.2, num.round=50,
eval.metric=mx.metric.accuracy)
mxPred <- predict(mnetModel, test.x)
View(train)
View(test)
#Hyperparameter Optimization using Deep Learning
library(h2o)
#hidden_opt <- list(c(100, 100, 100, 100), c(200, 200, 200, 200), c(300, 300, 300, 300))
hidden_opt <- list(c(5,5,5,5,5), c(10, 10, 10, 10), c(50, 50, 50, 50))
#l1_opt <- c(1e-5,1e-7)
#epochs = c(50, 100)
l1 = c(0, 0.00001, 0.0001)
l2 = c(0, 0.00001, 0.0001)
rate = c(0, 01, 0.005, 0.001)
# rho = c(0.9, 0.95, 0.99, 0.999)
#activations <- c("Tanh", "TanhWithDropout", "Rectifier", "RectifierWithDropout", "Maxout", "MaxoutWithDropout")
activations <- c("Rectifier", "Maxout", "Tanh", "RectifierWithDropout", "MaxoutWithDropout", "TanhWithDropout")
#hyper_params <- list(hidden = hidden_opt, l1 = l1_opt, activation=activations)
hyper_params <- list(hidden = hidden_opt, l1 = l1, activation=activations, l2 = l2, momentum_start = c(0, 0.5),
momentum_stable = c(0.99, 0.5, 0), rate = rate)
#hyper_params <- list(ntrees = c(1,300), learn_rate = c(0.1, 0.001))
h2o.init(ip = "localhost", port = 54321)
train$Label <- factor(train$Label)
h2otrain <- train
h2otrain <- as.h2o(h2otrain)
test$Label <- as.integer(test$Label)
h2otest <- test
h2otest <- as.h2o(h2otest)
model_grid <- h2o.grid("deeplearning",
hyper_params = hyper_params,
x = c(2:length(h2otrain)),  # column numbers for predictors
y = 1,   # column number for label
training_frame = h2otrain,
validation_frame = h2otest)
dlPerf <- c()
test$Label <- as.factor(test$Label)
h2otest <- test
h2otest <- as.h2o(h2otest)
model_grid <- h2o.grid("deeplearning",
hyper_params = hyper_params,
x = c(2:length(h2otrain)),  # column numbers for predictors
y = 1,   # column number for label
training_frame = h2otrain,
validation_frame = h2otest)
dlPerf <- c()
for (model_id in model_grid@model_ids){
model <- h2o.getModel(model_id)
pred <- h2o.predict(model, h2otest)
pred <- as.data.frame(pred)
dlPerformance <- 1 - mean(pred$predict != test$Label)
dlPerf <- rbind(dlPerf, dlPerformance)
}
#The best accuracy
(bestDL <- max(dlPerf))
str(test)
test$Label <- as.integer(test$Label)
str(test)
for (model_id in model_grid@model_ids){
model <- h2o.getModel(model_id)
pred <- h2o.predict(model, h2otest)
pred <- as.data.frame(pred)
dlPerformance <- 1 - mean(pred$predict != test$Label)
dlPerf <- rbind(dlPerf, dlPerformance)
}
#The best accuracy
(bestDL <- max(dlPerf))
mydata <- read.csv(file = file.choose(), header = T)
View(mydata)
plot(mydata$Death, xlab = 'Year',ylab = 'Count of Deaths', lwd = 2, col="Navy",main = "Deaths on each year")
barplot(mydata$Death, xlab = 'Year',ylab = 'Count of Deaths', lwd = 2, col="Navy",main = "Deaths on each year")
barplot(mydata$Death, xlab = 'Year',ylab = 'Count of Deaths',  col="Navy",main = "Deaths on each year")
barplot(mydata$Death, xlab = 'Year',ylab = 'Count of Deaths', ylim = c(0,60000) col="Navy",main = "Deaths on each year")
barplot(mydata$Death, xlab = 'Year',ylab = 'Count of Deaths', ylim = c(0,60000), col="Navy",main = "Deaths on each year")
barplot(mydata$Death, xlab = 'Year',ylab = 'Count of Deaths', ylim = c(10000,60000), col="Navy",main = "Deaths on each year")
barplot(mydata$Death, xlab = 'Year',ylab = 'Count of Deaths', ylim = c(10000,600000), col="Navy",main = "Deaths on each year")
barplot(mydata$Death, xlab = 'Year',ylab = 'Count of Deaths', ylim = c(0,600000), col="Navy",main = "Deaths on each year")
barplot(mydata$Death, xlab = 'Year',ylab = 'Count of Deaths', ylim = c(0,500000), col="Navy",main = "Deaths on each year")
barplot(mydata$Death, xlab = 'Year',ylab = 'Count of Deaths', col="Navy",main = "Deaths on each year")
mydata <- read.csv(file = file.choose(), header = T)
library(ggplot2)
barplot(mydata$Death, xlab = 'Year',ylab = 'Count of Deaths', col="Navy",main = "Deaths on each year")
barplot(mydata$Death, xlab = 'Year',ylab = 'Count of Deaths', xlim = c(1970, 2017), col="Navy",main = "Deaths on each year")
barplot(mydata$Death, xlab = 'Year',ylab = 'Count of Deaths', col="Navy",main = "Deaths on each year")
ggplot(mydata, aes(x = Year, y = Death)) +
xlab("Year") +
ylab("Count of Deaths")+
ggtitle("Deaths on each Year")
ggplot(mydata, aes(x = Year, y = Death)) +
geom_bar() +
xlab("Year") +
ylab("Count of Deaths")+
ggtitle("Deaths on each Year")
ggplot(mydata, aes(x = Year, y = Death)) +
geom_bar(stat = "count") +
xlab("Year") +
ylab("Count of Deaths")+
ggtitle("Deaths on each Year")
?geom_bar
ggplot(mydata, aes(x = Year, y = Death)) +
geom_bar(data = Year, stat = "count") +
xlab("Year") +
ylab("Count of Deaths")+
ggtitle("Deaths on each Year")
ggplot(mydata, aes(x = Year, y = Death)) +
geom_bar(data = mydata, stat = "count") +
xlab("Year") +
ylab("Count of Deaths")+
ggtitle("Deaths on each Year")
ggplot(mydata, aes(x = Year, y = Death)) +
geom_bar(data = mydata,mapping = aes(y = Death), stat = "count") +
xlab("Year") +
ylab("Count of Deaths")+
ggtitle("Deaths on each Year")
ggplot(mydata, aes(x = Year, y = Death)) +
geom_bar(data = mydata,mapping = aes(y = Death)) +
xlab("Year") +
ylab("Count of Deaths")+
ggtitle("Deaths on each Year")
ggplot(mydata, aes(x = Year, y = Death)) +
geom_point() +
xlab("Year") +
ylab("Count of Deaths")+
ggtitle("Deaths on each Year")
mydata <- read.csv(file = file.choose(), header = T)
View(mydata)
library(ggplot2)
ggplot(mydata, aes(x = Going, y = Place)) +
geom_point() +
xlab("Going") +
ylab("Places")
ggplot(mydata, aes(x = Going, y = Place, colour = Sex)) +
geom_point() +
xlab("Going") +
ylab("Places") +
facet_wrap( ~ Sex)
ggplot(mydata, aes(x = Place, y = Sex, colour = Going)) +
geom_point() +
xlab("Going") +
ylab("Places") +
facet_wrap( ~ Going)
ggplot(mydata, aes(x = Place, y = Sex, colour = Going)) +
geom_point() +
xlab("Place") +
ylab("Sex") +
facet_wrap( ~ Going)
ggplot(mydata, aes(x = mean(Place) , y = Sex, colour = Going)) +
geom_point() +
xlab("Place") +
ylab("Sex") +
facet_wrap( ~ Going)
df <- aggregate(mydata[, 2], list(mydata$Sex, mydata$Going), mean)
View(df)
ggplot(df, aes(x = Place , y = Sex, colour = Going)) +
geom_point() +
xlab("Place") +
ylab("Sex") +
facet_wrap( ~ Going)
ggplot(df, aes(x = x , y = Group.1, colour = Group.2)) +
geom_point() +
xlab("Place") +
ylab("Sex") +
facet_wrap( ~ Group.2)
ggplot(df, aes(x = x , y = Group.1, colour = Group.2)) +
geom_point() +
xlab("Wining Position") +
ylab("Sex") +
facet_wrap( ~ Group.2) +
ggtitle("Wining Position based on Sex and Going (Track condition)")
install.packages("quanteda")
install.packages("irlba")
library(quanteda)
library(irlba)
library(e1071)
library(caret)
setwd("C:/Study Materials/Machine Learning/heart-disease-uci/TextAnalytics-Classification")
#import the input raw data
raw.data <- read.csv("spam.csv", stringsAsFactors = FALSE)
View(raw.data)
#remove unwanted columns
raw.data <- raw.data[,1:2]
names(raw.data) <- c("Labe", "Text")
#import the input raw data
raw.data <- read.csv("spam.csv", stringsAsFactors = FALSE)
#remove unwanted columns
raw.data <- raw.data[,1:2]
names(raw.data) <- c("Label", "Text")
View(raw.data)
#check missing values
length(which(!complete.cases(raw.data)))
str(raw.data)
raw.data$Label <- as.factor(raw.data$Label)
#Class balance check
prop.table(table(raw.data$Label))
#Add a new column to have the length of the text
raw.data$TextLength <- nchar(raw.data$Text)
summary(raw.data)
View(raw.data)
#Visualize the distribution
library(ggplot2)
ggplot(raw.data, aes(x= TextLength, fill = Label)) +
theme_bw() +
geom_histogram(binwidth = 5) +
labs(y = "Text Count", x = "Length of the Text",
title = "Distribution of Text lengths")
#Create train and test data sets
library(caret)
set.seed(1234)
idx <- createDataPartition(raw.data$Label, times = 1, p = 0.7, list = FALSE)
train <- raw.data[idx,]
test <- raw.data[-idx,]
